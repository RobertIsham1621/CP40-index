{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZJJIFB4HUwA3sGUfXzHrw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertIsham1621/CP40-index/blob/main/cp40train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "illoq-Zmg_RO",
        "outputId": "d0a0d445-9db3-4a1e-eb4c-2f896e68e670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0003.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0005.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0006.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0008.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0009.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0012.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0013.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0014.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0015.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0016.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0017.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0018.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0019.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0020.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0021.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0022.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0024.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0025.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0026.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0027.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0028.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0029.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0033.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0037.JPG\n",
            "http://aalt.law.uh.edu/AALT1/H6/CP40no740/aCP40no740fronts/IMG_0038.JPG\n"
          ]
        }
      ],
      "source": [
        "# Import the requests and json modules\n",
        "import requests\n",
        "import json\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import urllib.request\n",
        "import secrets\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Create a directory\n",
        "try:\n",
        "  os.makedirs('/content/train')\n",
        "  os.makedirs('/content/train/images')\n",
        "  os.makedirs('/content/train/labels')\n",
        "  os.makedirs('/content/valid/images')\n",
        "  os.makedirs('/content/valid/labels')\n",
        "  \n",
        "except:\n",
        "  print('Problem')\n",
        "\n",
        "\n",
        "# Define the URL of the JSON file\n",
        "url = \"https://raw.githubusercontent.com/RobertIsham1621/CP40-index/main/CP40-740.json\"\n",
        "\n",
        "# Send a GET request to the URL and get the response\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the response as JSON\n",
        "data = response.json()\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to convert bounding box coordinates to region relative coordinates\n",
        "def bbox_to_region(bbox, image_width, image_height, grid_size):\n",
        "    # Convert (x, y, w, h) format to (cx, cy, w, h) format\n",
        "    cx = bbox[0] + bbox[2] / 2\n",
        "    cy = bbox[1] + bbox[3] / 2\n",
        "    w = bbox[2]\n",
        "    h = bbox[3]\n",
        "    \n",
        "    # Normalize the coordinates by dividing them by the image width and height\n",
        "    cx /= image_width\n",
        "    cy /= image_height\n",
        "    w /= image_width\n",
        "    h /= image_height\n",
        "    \n",
        "    # Assign each box to a grid cell based on its center coordinates\n",
        "    grid_x = int(cx * grid_size)\n",
        "    grid_y = int(cy * grid_size)\n",
        "    \n",
        "    # Subtract the top-left coordinates of the grid cell from the normalized center coordinates of the box\n",
        "    cx -= grid_x / grid_size\n",
        "    cy -= grid_y / grid_size\n",
        "    \n",
        "    # Return the region relative coordinates as a numpy array\n",
        "    return np.array([cx, cy, w, h])\n",
        "\n",
        "# Print the data or store it in a variable\n",
        "count=0\n",
        "imgs_loaded=set()\n",
        "classdict=dict()\n",
        "classind=0\n",
        "which=1\n",
        "for img in data['_via_img_metadata']:\n",
        "  imgurl=img[0:-2]\n",
        "  try:\n",
        "    file =[] \n",
        "    for r in  data['_via_img_metadata'][img]['regions']:\n",
        "      if 'field_type' in r['region_attributes']:\n",
        "        if r['region_attributes']['field_type'] in\\\n",
        "        ['letter','latin phrase']:\n",
        "          if not file:\n",
        "            print(imgurl)\n",
        "            file=BytesIO(urllib.request.urlopen(imgurl).read())\n",
        "          if which == 1:\n",
        "            which =2\n",
        "          elif which == 2:\n",
        "            which =1 \n",
        "          with Image.open(file) as im:\n",
        "            # Set the box tuple (left, upper, right, lower)\n",
        "            x1=r['shape_attributes']['x']\n",
        "            x2=r['shape_attributes']['x']+r['shape_attributes']['width']\n",
        "            w=r['shape_attributes']['width']\n",
        "            y1=r['shape_attributes']['y']\n",
        "            y2=r['shape_attributes']['y']+r['shape_attributes']['height']   \n",
        "            h=r['shape_attributes']['height']\n",
        "            box = (x1, y1, x2, y2)\n",
        "\n",
        "            alphabet = string.ascii_letters + string.digits\n",
        "            password = ''.join(secrets.choice(alphabet) for i in range(8))\n",
        "            # Crop the image\n",
        "            # Save the cropped image\n",
        "    # Crop the image\n",
        "            im_cropped = im.crop(box)\n",
        "            if which == 1:\n",
        "              im_cropped.save(\"train/images/\"+password + '.jpg')\n",
        "            else:\n",
        "              im_cropped.save(\"valid/images/\"+password + '.jpg')\n",
        "            label=r['region_attributes']['text']\n",
        "            if label in classdict:\n",
        "              classnum=classdict[label]\n",
        "            else:\n",
        "              classdict[label]=classind\n",
        "              classnum=classdict[label]\n",
        "              classind +=1\n",
        "            reg=[classnum,0.5, 0.5, 1,1]\n",
        "            if which == 1:\n",
        "              mylabelfile=\"train/labels/\"+password + '.txt'\n",
        "            else:\n",
        "              mylabelfile =\"valid/labels/\"+password + '.txt'\n",
        "            with open(mylabelfile,'a') as f:\n",
        "              f.writelines(str(reg[0]) +\" \"+ str(reg[1])+\" \"+str(reg[2])+\" \"+str(reg[3]) +\" \"+str(reg[4]) +\"\\n\")\n",
        "\n",
        "      if 'fully_indexed' in r['region_attributes']:\n",
        "        if 'yes' in r['region_attributes']['fully_indexed']:\n",
        "          if not file:\n",
        "            file=BytesIO(urllib.request.urlopen(imgurl).read())\n",
        "          if which == 1:\n",
        "            which =2\n",
        "          elif which == 2:\n",
        "            which =1 \n",
        "          if r['region_attributes']['fully_indexed']['yes']:\n",
        "            x1=r['shape_attributes']['x']\n",
        "            x2=r['shape_attributes']['x']+r['shape_attributes']['width']\n",
        "            w=r['shape_attributes']['width']\n",
        "            y1=r['shape_attributes']['y']\n",
        "            y2=r['shape_attributes']['y']+r['shape_attributes']['height']   \n",
        "            h=r['shape_attributes']['height']\n",
        "            imgurl=img[0:-2]\n",
        "            file = BytesIO(urllib.request.urlopen(imgurl).read())\n",
        "            with Image.open(file) as im:\n",
        "              # Set the box tuple (left, upper, right, lower)\n",
        "              box = (x1, y1, x2, y2)\n",
        "\n",
        "              alphabet = string.ascii_letters + string.digits\n",
        "              password = ''.join(secrets.choice(alphabet) for i in range(8))\n",
        "              # Crop the image\n",
        "              # Save the cropped image\n",
        "      # Crop the image\n",
        "              im_cropped = im.crop(box)\n",
        "              if which == 1:\n",
        "                im_cropped.save(\"train/images/\"+password + '.jpg')\n",
        "              else:\n",
        "                im_cropped.save(\"valid/images/\"+password + '.jpg')\n",
        "              for r2 in  data['_via_img_metadata'][img]['regions']:\n",
        "                xr1=r2['shape_attributes']['x']\n",
        "                xr2=r2['shape_attributes']['x']+r2['shape_attributes']['width']\n",
        "                w2=r2['shape_attributes']['width']\n",
        "                yr1=r2['shape_attributes']['y']\n",
        "                yr2=r2['shape_attributes']['y']+r2['shape_attributes']['height'] \n",
        "                h2=r2['shape_attributes']['height'] \n",
        "                if xr1>x1 and xr2<x2 and yr1>y1 and yr2<y2:\n",
        "                  if 'field_type' in r2['region_attributes']:\n",
        "                    if r2['region_attributes']['field_type'] in\\\n",
        "                    ['letter','county','latin phrase']:\n",
        "                          \n",
        "                      label=r2['region_attributes']['text']\n",
        "                      if label in classdict:\n",
        "                        classnum=classdict[label]\n",
        "                      else:\n",
        "                        classdict[label]=classind\n",
        "                        classnum=classdict[label]\n",
        "                        classind +=1\n",
        "                      reg=[classnum,((xr1+xr2)/2-x1)/w, ((yr1+yr2)/2-y1)/h, w2/w,h2/h]\n",
        "                      if which == 1:\n",
        "                        mylabelfile=\"train/labels/\"+password + '.txt'\n",
        "                      else:\n",
        "                        mylabelfile =\"valid/labels/\"+password + '.txt'\n",
        "                      with open(mylabelfile,'a') as f:\n",
        "                        f.writelines(str(reg[0]) +\" \"+ str(reg[1])+\" \"+str(reg[2])+\" \"+str(reg[3]) +\" \"+str(reg[4]) +\"\\n\")\n",
        "  except:\n",
        "    print('prolem')\n",
        "with open('data.yaml', 'w') as file:\n",
        "    file.write('train: ../train/images \\n')\n",
        "    file.write('val: ../valid/images \\n')\n",
        "    file.write('test: ../test/images \\n')\n",
        "    file.write('nc: ' + str(len(classdict.keys()))+ '\\n')\n",
        "    file.write('names: '+ str(list(classdict.keys())) + '\\n')\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2JRQWyDhOuH",
        "outputId": "64345212-80e7-49e2-c74e-dad8cfef0dfe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.59 🚀 Python-3.9.16 torch-1.13.1+cu116 CPU\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 25.6/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo train model=yolov8s.pt data=data.yaml epochs=200  rect=True fliplr=0.0 degrees=15\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Id-aoYfhRZ6",
        "outputId": "f8c854c2-ffd6-4ef9-8e5b-0cb772f42f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to yolov8s.pt...\n",
            "100% 21.5M/21.5M [00:00<00:00, 62.1MB/s]\n",
            "Ultralytics YOLOv8.0.59 🚀 Python-3.9.16 torch-1.13.1+cu116 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=data.yaml, epochs=200, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=True, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=15, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.Unicode.ttf to /root/.config/Ultralytics/Arial.Unicode.ttf...\n",
            "100% 22.2M/22.2M [00:00<00:00, 257MB/s]\n",
            "Overriding model.yaml nc=80 with nc=62\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.Conv                  [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.C2f                   [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.C2f                   [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.C2f                   [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.C2f                   [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.C2f                   [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2140042  ultralytics.nn.modules.Detect                [62, [128, 256, 512]]         \n",
            "Model summary: 225 layers, 11159594 parameters, 11159578 gradients, 28.8 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "2023-04-02 19:54:26.867156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-02 19:54:27.803968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
            "WARNING ⚠️ 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/train/labels... 350 images, 0 backgrounds, 13 corrupt: 100% 363/363 [00:00<00:00, 2636.54it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/0Gjx9qGf.jpg: ignoring corrupt image/label: image size (16, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/1dY4T9qs.jpg: ignoring corrupt image/label: image size (37, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/5tjwUsIm.jpg: ignoring corrupt image/label: image size (18, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/6tUEKiqP.jpg: ignoring corrupt image/label: image size (24, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/B86xB9Jl.jpg: ignoring corrupt image/label: image size (17, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/Uhjo1fdv.jpg: ignoring corrupt image/label: image size (26, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/cqA04sFs.jpg: ignoring corrupt image/label: image size (24, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/gGN5Amv3.jpg: ignoring corrupt image/label: image size (22, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/pMRJE410.jpg: ignoring corrupt image/label: image size (33, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/tJrpwMhn.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/v9IS5URJ.jpg: ignoring corrupt image/label: image size (18, 6) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/yNM0B8KW.jpg: ignoring corrupt image/label: image size (21, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/train/images/yrdBaQG0.jpg: ignoring corrupt image/label: image size (22, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/valid/labels... 354 images, 0 backgrounds, 9 corrupt: 100% 363/363 [00:00<00:00, 3396.44it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/BLgU8KOT.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/G8ucRI14.jpg: ignoring corrupt image/label: image size (19, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/H6dQ2mTC.jpg: ignoring corrupt image/label: image size (27, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/STXkfVn1.jpg: ignoring corrupt image/label: image size (28, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/XAFTx9w3.jpg: ignoring corrupt image/label: image size (22, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/aogs3kYN.jpg: ignoring corrupt image/label: image size (44, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/b0YTlhEl.jpg: ignoring corrupt image/label: image size (20, 8) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/d40gwuGC.jpg: ignoring corrupt image/label: image size (17, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/valid/images/zZBqFtyG.jpg: ignoring corrupt image/label: image size (34, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 200 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/200         0G      3.135      6.032      2.138        236        640:   5% 1/22 [00:12<04:12, 12.01s/it]Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\n",
            "100% 755k/755k [00:00<00:00, 43.6MB/s]\n",
            "      1/200         0G      2.352      9.377      2.421         16        640:  18% 4/22 [01:08<05:38, 18.79s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo mode=predict  model=runs/detect/train/weights/best.pt source=real_test.jpeg save_txt=True conf=0.2  save=True rec"
      ],
      "metadata": {
        "id": "ZIhr2_qyhqmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}